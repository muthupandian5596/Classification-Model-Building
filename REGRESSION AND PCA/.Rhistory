# Regression Equation y = 4.0541 + 0.6695 (OrdBilling)
# Variance Explained by the model is 27% (0.2722)
# Order and Billing is highly Significant @ 100% confidence Interval
# Order and Billing also has high impact on dependent variable Satisfaction
model11 <- lm(Satisfaction~DelSpeed)
# Inference:
# Regression Equation y = 3.2791 + 0.9364 (Delivery Speed)
# Variance Explained by the model is 33% (0.333)
# Delivery Speed is highly Significant @ 100% confidence Interval
# Delivery Speed also has high impact on dependent variable Satisfaction
## Inference Out of all the Models
# Technical Support doesn't have much impact on the Satisfaction level of the customer
# All other independent Variables have significant impact on the dependent variable
# NOtably, DelSpeed,SalesFimage,ProdQual,CompRes,ProdLine are Highly Significant and Impactfull Vairable
# Since the pressence of multi-collinearity is evident , its time to act on it using one of the methods
# PCA/ Factor Analysis
# 1st Step: the correlation matrix to check
Cor <- cor(Hair[,c(-1,-13)])
print(Cor)
# Its clear that all the variables are highly corelated with each other as we already saw it while checking multi-collinearity
library("Hmisc")
# Let's check the correlation matrix with p-values
#rcorr <- rcorr(as.matrix(Hair))
#print(rcorr)
#Hair.coeff <- rcorr$r
#print(Hair.coeff)
#Hair.p <- rcorr$P
#print(Hair.p)
library(corrplot)
corrplot(Cor,method = "circle",type = "upper")
# Inference From Correlation Plot
# Correlation coefficients greater than 0.3 in absolute value are indicative of acceptable correlations.
# Product Quality and Product Line are positively Correlated (0.477)
# Product Quality and Competitive Pricing are negatively Correlated (-0.401)
# Ecom and Sales Force Image are very highly positively correlated (0.792)
# Ecom and Advertising are  positively correlated (0.429)
# Technical Support and Warranty Claim  are Highly Positively Correlated (0.797)
# Complaint Resolution and Delivery Speed  are highly positively correlated (0.865)
# Complaint Resolution and Order Billing are Highly positively Correlated (0.757)
# Complaint Resolution and Product Line are positively Correlated (0.561)
# Advertising and Sales Force Image are positively Correlated (0.542)
# Product Line and Competitive Pricing are negatively Correlated (-0.495)
# Product Line and Delivery Speed are positively Correlated (0.601)
# Product Line and Order & Billing are positively Correlated (0.424)
# Order Billing and Delivery Speed are Highly positively Correlated (0.751)
## Step 2 - Factor Extraction using Kaizer Rule
# Based on Correlation Matrix we confirmed the presence of Correlation and there is need for Factor Analysis
# Now we need to figure out how many Factors are needed using scree plot
# To decide on how many factors we need to represent the data, we use 2 statistical criteria: Eigen Values,and The Scree Plot.
library(nFactors)
# Eigen Value Computation
ev <- eigen(cor(Cor))
print(ev)
EIgenValue <- ev$values
print(EIgenValue)
# The determination of the number of factors is usually done by considering only factors with Eigen values greater than 1.
# Factors with a variance less than 1 are no better than a single variable, since each variable is expected to have a variance of 1.
# Here we have 3 eigen Values Greater than 1 so we can have 3 Factors.
# Lets check it with scree plot also.
Factor<- c(1,2,3,4,5,6,7,8,9,10,11)
scree <- data.frame(Factor,EIgenValue)
plot(scree,main="Scree Plot",col = "Blue",ylim=c(0,6))
lines(scree,col="Red")
# In Scree Plot we can clearly see the decreasing trend in the Eigen Value as the Factor Increases.
# As per Kaizer Normalisation Rule "Any Eigen Value less than 1 is not a much of a Value to be considered"
# So as per this rule we can omit Eigen Values which are Less than 1.
# i.e.,we have 4 Factors to be extracted (Given in Problem Statement)
# Now lets find out the Component matrix using Principle Component Analysis
library(psych)
Unrotate <- principal(Hair[,c(-1,-13)],nfactors = 4,rotate = "none")
print(Unrotate,digits = 3)
# Insights
# Eigen Value(SS Loadings) represents the largest Variance Summarized / reduction
# Proportion Variance explains how much variance is explained by each Component
# PC1 explains 31.2%,PC2 explains 23.2%, PC3 explains 15.4%, PC4 explains 9.9%
# Cummulative Proportion explains total Variance explained by all the components
# Totally we have explained 100 % of information with 4 Variables.
# Communality is the common Variance captured by all the Factors
# Communality gives how much variance explained by all the 4 Factors of every Variable.
# eg.,91.4% of variance of Delivery Speed is explained by all the 4 Factors
# The Interpretability of Components in Unrotated Factor Loading is Difficult as many variables have High Loading(Corelated) in each Factors
# It creates a amalgamation Effect of Variables in all the Factor
# Step 3 - Factor Rotation
# Un-rotated factors are typically not very interpretable (most factors are correlated with many variables).
# Factors are rotated to make them more meaningful and easier to interpret (each variable is associated with a minimal number of factors).
# Different rotation methods may result in the identification of somewhat different factors.
# The most popular rotational method is Varimax rotations.
# Varimax use orthogonal rotations yielding uncorrelated factors/components.
# Varimax attempts to minimize the number of variables that have high loadings on a factor. This enhances the interpretability of the factors.
Rotate <- principal(Hair[,c(-1,-13)],nfactors = 4,rotate = "varimax")
print(Rotate,digits = 3)
#  Inference
# After rotation,To identify factors, group variables that have larger loadings for the same factor.
# and then name the factors based on the meaning of Variables
# the Factor Names are Purely Intuitional
# Variables like Complaint Resolution(0.926),Order and Billing(0.864),Delivery Speed (0.938) are well explained by the Customer Service Factor(RC1)
# Variables like E-Commerce(0.871),SalesForceImage(0.900),Advertising(0.742),competitive pricing(0.226) are well explained by the Marketing Factor (RC2)
# Variables like Technical Support(0.939),warranty claim(0.931) are well explained by the Customer Support Factor (RC3)
# Variables like Product Quality(0.876),Product Line(0.642) are well explained by Product Factor (RC4)
# Let's Visually Plot the Loadings and cross check our Inference
plot(Rotate,row.names(Rotate$loadings),cex=1.0)
summary(Ecom)
summary(Hair)
summary(Ecom)
summary(Hair$Ecom)
attach(Hair)
summary(Ecom)
summary(Hair$Ecom)
Ecom[Ecom>4.9]
Ecom[Ecom>4.9,]
Ecom[Ecom>4.9]
Hair$Ecom[Hair$Ecom>4.9]
summary(SalesFImage)
SalesFImage[SalesFImage>7.1]
# From the Plot its Clear that the Variables E-Commerce,Sales Force Image,Order and Billing,Delivery Speed has Outliers,
# Which we also assumed in Histogram Plot Itself.
# We need to deal with the outlier as it will affect our Model interprettability/Validity.
# We need to find the Outlier values using the formula (Q1-1.5IQR,Q1+1.5IQR)
# There are different methods in dealing with Outliers
# One Of the method is Winsorizing technique which replaces the outliers with the allowable Range(Q1-1.5IQR,Q1+1.5IQR).
#(4.9)
summary(Ecom)
# From the Plot its Clear that the Variables E-Commerce,Sales Force Image,Order and Billing,Delivery Speed has Outliers,
# Which we also assumed in Histogram Plot Itself.
# We need to deal with the outlier as it will affect our Model interprettability/Validity.
# We need to find the Outlier values using the formula (Q1-1.5IQR,Q1+1.5IQR)
# There are different methods in dealing with Outliers
# One Of the method is Winsorizing technique which replaces the outliers with the allowable Range(Q1-1.5IQR,Q1+1.5IQR).
#(4.9)
summary(Hair$Ecom)
?iqr
IQR
benchmark <- 3.925+1.5(IQR(Hair$Ecom))
benchmark <- 3.925+1.5*IQR(Hair$Ecom)
print(benchmark)
Hair$Ecom[Hair$Ecom>4.9]
Hair$Ecom[Hair$Ecom>4.9] <- benchmark
summary(Ecom)
summary(Hair$Ecom)
boxplot(Ecom)
boxplot(Hair$Ecom)
boxplot(Hair$Ecom,col = "orange")
# We have replaced the Outliers with acceptable or allowable range which is evident in the boxplot
# Now Lets do the same process for all other variables which has outliers
summary(SalesFImage)
benchmark <- 5.800+1.5*IQR(SalesFImage)
print(benchmark)
SalesFImage[SalesFImage>7.75]
SalesFImage[SalesFImage>7.75] <- benchmark
summary(SalesFImage)
boxplot(SalesFImage,col = "Blue")
boxplot(SalesFImage,col = "cyan")
summary(Hair$Ecom)
benchmark <- 3.925+1.5*IQR(Hair$Ecom)
print(benchmark)
# Benchmark is the upper allowable Limit
Hair$Ecom[Hair$Ecom>4.9] <- benchmark
summary(Hair$Ecom)
boxplot(Hair$Ecom,col = "orange")
# We have replaced the Outliers with acceptable or allowable range which is evident in the boxplot
# Now Lets do the same process for all other variables which has outliers
summary(SalesFImage)
benchmark <- 5.800+1.5*IQR(SalesFImage)
print(benchmark)
SalesFImage[SalesFImage>7.75] <- benchmark
summary(SalesFImage)
boxplot(SalesFImage,col = "cyan")
# From the Plot its Clear that the Variables E-Commerce,Sales Force Image,Order and Billing,Delivery Speed has Outliers,
# Which we also assumed in Histogram Plot Itself.
# We need to deal with the outlier as it will affect our Model interprettability/Validity.
# We need to find the Outlier values using the formula (Q1-1.5IQR,Q1+1.5IQR)
# There are different methods in dealing with Outliers
# One Of the method is Winsorizing technique which replaces the outliers with the allowable or Benchmark Range(Q1-1.5IQR,Q1+1.5IQR).
summary(Hair$Ecom)
benchmark <- 3.925+1.5*IQR(Hair$Ecom)
print(benchmark)
# Benchmark is the upper allowable Limit
Hair$Ecom[Hair$Ecom>4.9] <- benchmark
summary(Hair$Ecom)
boxplot(Hair$Ecom,col = "orange")
# We have replaced the Outliers with acceptable or allowable range which is evident in the boxplot
# Now Lets do the same process for all other variables which has outliers
summary(SalesFImage)
benchmark1 <- 5.800+1.5*IQR(SalesFImage)
print(benchmark1)
SalesFImage[SalesFImage>7.75] <- benchmark1
summary(SalesFImage)
boxplot(SalesFImage,col = "cyan")
summary(OrdBilling)
summary(OrdBilling)
benchmark2 <- 4.800+1.5*IQR(OrdBilling)
print(benchmark2)
OrdBilling[OrdBilling>6.45] <- benchmark2
l.benchmark2 <- 3.700-1.5*IQR(OrdBilling)
print(l.benchmark2)
OrdBilling[OrdBilling<2.05] <- l.benchmark2
summary(SalesFImage)
summary(OrdBilling)
boxplot(OrdBilling,col = "Purple")
summary(DelSpeed)
l.benchmark3 <- 3.400+1.5*IQR(DelSpeed)
print(l.benchmark3)
l.benchmark3 <- 3.400-1.5*IQR(DelSpeed)
print(l.benchmark3)
DelSpeed[DelSpeed<1.8625] <- l.benchmark3
summary(DelSpeed)
boxplot(DelSpeed,col = "Pink")
boxplot(DelSpeed,col = "Rose")
boxplot(DelSpeed,col = "Dark Pink")
boxplot(DelSpeed,col = "Pink")
boxplot(Hair[,2:13],col=rainbow(length(Hair[,2:13])),las = 2)
summary(Hair$Ecom)
benchmark <- 3.925+1.5*IQR(Hair$Ecom)
print(benchmark)
# Benchmark is the upper allowable Limit
Hair$Ecom[Hair$Ecom>4.9] <- benchmark
summary(Hair$Ecom)
boxplot(Hair$Ecom,col = "orange")
# We have replaced the Outliers with acceptable or allowable range which is evident in the boxplot
# Now Lets do the same process for all other variables which has outliers
summary(SalesFImage)
benchmark1 <- 5.800+1.5*IQR(SalesFImage)
print(benchmark1)
SalesFImage[SalesFImage>7.75] <- benchmark1
summary(SalesFImage)
boxplot(SalesFImage,col = "cyan")
summary(OrdBilling)
benchmark2 <- 4.800+1.5*IQR(OrdBilling)
print(benchmark2)
OrdBilling[OrdBilling>6.45] <- benchmark2
l.benchmark2 <- 3.700-1.5*IQR(OrdBilling)
print(l.benchmark2)
# l.benchmark2 means lower benchmark/ acceptable range as Order & Billing has outliers in both sides
OrdBilling[OrdBilling<2.05] <- l.benchmark2
summary(OrdBilling)
boxplot(OrdBilling,col = "Purple")
summary(DelSpeed)
l.benchmark3 <- 3.400-1.5*IQR(DelSpeed)
print(l.benchmark3)
DelSpeed[DelSpeed<1.8625] <- l.benchmark3
summary(DelSpeed)
boxplot(DelSpeed,col = "Pink")
boxplot(Hair[,2:13],col=rainbow(length(Hair[,2:13])),las = 2)
summary(Hair$DelSpeed)
summary(DelSpeed)
attach(Hair)
# Understanding the data
# Structure of Data
str(Hair)
# Outlier Detection
par(mfrow=c(1,1))
boxplot(Hair[,2:13],col=rainbow(length(Hair[,2:13])),las=2)
# From the Plot its Clear that the Variables E-Commerce,Sales Force Image,Order and Billing,Delivery Speed has Outliers,
# Which we also assumed in Histogram Plot Itself.
# We need to deal with the outlier as it will affect our Model interprettability/Validity.
# We need to find the Outlier values using the formula (Q1-1.5IQR,Q1+1.5IQR)
# Outlier Treatment
# There are different methods in dealing with Outliers
# One Of the method is Winsorizing technique which replaces the outliers with the allowable or Benchmark Range(Q1-1.5IQR,Q1+1.5IQR).
summary(Hair$Ecom)
benchmark <- 3.925+1.5*IQR(Hair$Ecom)
print(benchmark)
# Benchmark is the upper allowable Limit
Hair$Ecom[Hair$Ecom>4.9] <- benchmark
summary(Hair$Ecom)
boxplot(Hair$Ecom,col = "orange")
# We have replaced the Outliers with acceptable or allowable range which is evident in the boxplot
# Now Lets do the same process for all other variables which has outliers
summary(SalesFImage)
benchmark1 <- 5.800+1.5*IQR(SalesFImage)
print(benchmark1)
SalesFImage[SalesFImage>7.75] <- benchmark1
summary(SalesFImage)
boxplot(SalesFImage,col = "cyan")
summary(OrdBilling)
benchmark2 <- 4.800+1.5*IQR(OrdBilling)
print(benchmark2)
OrdBilling[OrdBilling>6.45] <- benchmark2
l.benchmark2 <- 3.700-1.5*IQR(OrdBilling)
print(l.benchmark2)
# l.benchmark2 means lower benchmark/ acceptable range as Order & Billing has outliers in both sides
OrdBilling[OrdBilling<2.05] <- l.benchmark2
summary(OrdBilling)
boxplot(OrdBilling,col = "Purple")
summary(DelSpeed)
print(l.benchmark3)
DelSpeed[DelSpeed<1.8625] <- l.benchmark3
summary(DelSpeed)
l.benchmark3 <- 3.400-1.5*IQR(DelSpeed)
boxplot(DelSpeed,col = "Pink")
boxplot(Hair[,2:13],col=rainbow(length(Hair[,2:13])),las = 2)
# Now Let's Check Whether there is evidence of Multicollinearity
library(corrplot)
Cor <- cor(Hair[,c(-1,-13)])
print(Cor)
boxplot(Hair[,2:13],col=rainbow(length(Hair[,2:13])),las = 2)
summary(Hair$SalesFImage)
benchmark1 <- 5.800+1.5*IQR(Hair$SalesFImage)
print(benchmark1)
Hair$SalesFImage[Hair$SalesFImage>7.75] <- benchmark1
summary(Hair$SalesFImage)
boxplot(Hair$SalesFImage,col = "cyan")
summary(Hair$OrdBilling)
benchmark2 <- 4.800+1.5*IQR(Hair$OrdBilling)
print(benchmark2)
Hair$OrdBilling[Hair$OrdBilling>6.45] <- benchmark2
l.benchmark2 <- 3.700-1.5*IQR(Hair$OrdBilling)
print(l.benchmark2)
# l.benchmark2 means lower benchmark/ acceptable range as Order & Billing has outliers in both sides
Hair$OrdBilling[Hair$OrdBilling<2.05] <- l.benchmark2
summary(Hair$OrdBilling)
boxplot(Hair$OrdBilling,col = "Purple")
summary(Hair$DelSpeed)
l.benchmark3 <- 3.400-1.5*IQR(Hair$DelSpeed)
print(l.benchmark3)
Hair$DelSpeed[Hair$DelSpeed<1.8625] <- l.benchmark3
summary(Hair$DelSpeed)
boxplot(Hair$DelSpeed,col = "Pink")
boxplot(Hair[,2:13],col=rainbow(length(Hair[,2:13])),las = 2)
# Lets build a simple Linear Regression model for the dependent Variable with every independent variable
# and interpret the significance
model1 <- lm(Satisfaction~ProdQual)
summary(model1)
# Lets build a simple Linear Regression model for the dependent Variable with every independent variable
# and interpret the significance
model1 <- lm(Hair$Satisfaction~Hair$ProdQual)
summary(model1)
# Inference:
# Regression Equation y = 3.67593 + 0.41512 (Product Quality)
# Variance Explained by the model is 23% (0.2365)
# Product Quality is highly Significant @ 100% confidence Interval
# Product Quality has high impact on dependent variable Satisfaction
model2 <- lm(Satisfaction~Ecom)
# Inference:
# Regression Equation y = 4.02203 + 0.49887 (ProdLine)
# Variance Explained by the model is 30% (0.3031)
# Product Line is highly Significant @ 100% confidence Interval
# Product Line also has high impact on dependent variable Satisfaction
model7 <- lm(Satisfaction~SalesFImage)
summary(model7)
model10 <- lm(Satisfaction~OrdBilling)
summary(model10)
model11 <- lm(Satisfaction~DelSpeed)
summary(model11)
# Inference:
# Regression Equation y = 3.67593 + 0.41512 (Product Quality)
# Variance Explained by the model is 23% (0.2365)
# Product Quality is highly Significant @ 100% confidence Interval
# Product Quality has high impact on dependent variable Satisfaction
model2 <- lm(Satisfaction~Ecom)
# Inference:
# Regression Equation y = 3.67593 + 0.41512 (Product Quality)
# Variance Explained by the model is 23% (0.2365)
# Product Quality is highly Significant @ 100% confidence Interval
# Product Quality has high impact on dependent variable Satisfaction
model2 <- lm(Satisfaction~Hair$Ecom)
summary(model2)
Rotate$scores
mydata <- Rotate$scores,Satisfaction
mydata <- Rotate$scores[,Satisfaction]
mydata <- Rotate$scores
mydata$Satisfaction <- Satisfaction
mydata <- data.frame(Rotate$scores,Satisfaction)
View(mydata)
View(mydata)
View(Hair)
View(mydata)
MLR <- data.frame(Rotate$scores,Satisfaction)
View(model4)
MLR.model <- lm(MLR$Satisfaction~RC1+RC2+RC3+RC4,data = MLR)
summary(MLR.model)
plot(MLR.model)
plot(MLR.model)
summary(MLR.model)
# Labelling the Factors
# Factor 1 is strongly correlated with Complaint Resolution(0.926),Order and Billing(0.864),Delivery Speed (0.938)
# This means there are people who care about the "Customer Services" . Therefore we can Call it Customer Service Factor.
# In Factor 2 is more correlated with E-Commerce(0.871),SalesForceImage(0.900),Advertising(0.742),competitive pricing(0.226)
# This means there are people who care about the "Marketing Strategies" . Therefore we can Call it Marketing Factor.
# Let's Visually Plot the Loadings and cross check our Inference
# In Factor 3 is explains a lot about Technical Support(0.939),warranty claim(0.931)
# This means there are people who care about the "Customer Support" . Therefore we can Call it Customer Support Factor.
# In Factor 4 is explains more about Product Quality(0.876),Product Line(0.642)
# This means there are people who care about the "Product and its quality" . Therefore we can Call it Product Factor.
plot(Rotate,row.names(Rotate$loadings),cex=1.0)
plot(model1)
summary(MLR.model)
annova(MLR.model)
anova(MLR.model)
summary(lm(MLR$Satisfaction~RC1+RC2+RC4,data = MLR))
plot(lm(MLR$Satisfaction~RC1+RC2+RC4,data = MLR))
# R-Square Interpretation
# Multiple R- Squared - 0.6605 implies 66.05% of the Variation in Satisfaction Rating  is explained  by the 4 Factor Scores.
# Overall Significance of the Model (Is there a Regression in the Population)
# Regression has 4 DF and Total has 99 DF.Hence error or residual has 99-4 = 95 DF
# Probability(F>46.21) = 2.2e-16(P-Value) is much smaller than alpha of 5% level
# Hence Reject the Null hypothesis of all betas are zero.Conclude atleast one beta is non-zero and hence accept the alternative hypothesis.
# Therefore Overall there is Overwhelming Evidence that regression model exist in the Population.
# Individual Coefficients are highly significant except Customer Support(RC3) as evidence by the T-Stats thats have extremely low P Values each one of them except Customer Support is much less than alpha of 5%
# All Factors Considered the Regression Model is Robust and can be used for Predicitive Analytics
# Confidence Interval
confint(MLR.model,"Satisfaction")
# R-Square Interpretation
# Multiple R- Squared - 0.6605 implies 66.05% of the Variation in Satisfaction Rating  is explained  by the 4 Factor Scores.
# Overall Significance of the Model (Is there a Regression in the Population)
# Regression has 4 DF and Total has 99 DF.Hence error or residual has 99-4 = 95 DF
# Probability(F>46.21) = 2.2e-16(P-Value) is much smaller than alpha of 5% level
# Hence Reject the Null hypothesis of all betas are zero.Conclude atleast one beta is non-zero and hence accept the alternative hypothesis.
# Therefore Overall there is Overwhelming Evidence that regression model exist in the Population.
# Individual Coefficients are highly significant except Customer Support(RC3) as evidence by the T-Stats thats have extremely low P Values each one of them except Customer Support is much less than alpha of 5%
# All Factors Considered the Regression Model is Robust and can be used for Predicitive Analytics
# Confidence Interval
confint(MLR.model,"MLR$Satisfaction")
confint(MLR.model)
# R-Square Interpretation
# Multiple R- Squared - 0.6605 implies 66.05% of the Variation in Satisfaction Rating  is explained  by the 4 Factor Scores.
# Overall Significance of the Model (Is there a Regression in the Population)
# Regression has 4 DF and Total has 99 DF.Hence error or residual has 99-4 = 95 DF
# Probability(F>46.21) = 2.2e-16(P-Value) is much smaller than alpha of 5% level
# Hence Reject the Null hypothesis of all betas are zero.Conclude atleast one beta is non-zero and hence accept the alternative hypothesis.
# Therefore Overall there is Overwhelming Evidence that regression model exist in the Population.
# Individual Coefficients are highly significant except Customer Support(RC3) as evidence by the T-Stats thats have extremely low P Values each one of them except Customer Support is much less than alpha of 5%
# All Factors Considered the Regression Model is Robust and can be used for Predicitive Analytics
# Confidence Interval
confint(MLR.model,"RC1")
confint(MLR.model,"RC2")
Rotate
# eg.,91.4% of variance of Delivery Speed is explained by all the 4 Factors
# The Interpretability of Components in Unrotated Factor Loading is Difficult as many variables have High Loading(Corelated) in each Factors
# It creates a amalgamation Effect of Variables in all the Factor
# Step 3 - Factor Rotation
# Un-rotated factors are typically not very interpretable (most factors are correlated with many variables).
# Factors are rotated to make them more meaningful and easier to interpret (each variable is associated with a minimal number of factors).
# Different rotation methods may result in the identification of somewhat different factors.
# The most popular rotational method is Varimax rotations.
# Varimax use orthogonal rotations yielding uncorrelated factors/components.
# Varimax attempts to minimize the number of variables that have high loadings on a factor. This enhances the interpretability of the factors.
Rotate <- principal(Hair[,c(-1,-13)],nfactors = 4,rotate = "varimax")
print(Rotate,digits = 3)
Unrotate
# eg.,91.4% of variance of Delivery Speed is explained by all the 4 Factors
# The Interpretability of Components in Unrotated Factor Loading is Difficult as many variables have High Loading(Corelated) in each Factors
# It creates a amalgamation Effect of Variables in all the Factor
# Step 3 - Factor Rotation
# Un-rotated factors are typically not very interpretable (most factors are correlated with many variables).
# Factors are rotated to make them more meaningful and easier to interpret (each variable is associated with a minimal number of factors).
# Different rotation methods may result in the identification of somewhat different factors.
# The most popular rotational method is Varimax rotations.
# Varimax use orthogonal rotations yielding uncorrelated factors/components.
# Varimax attempts to minimize the number of variables that have high loadings on a factor. This enhances the interpretability of the factors.
Rotate <- principal(Hair[,c(-1,-13)],nfactors = 4,rotate = "varimax")
print(Rotate,digits = 3)
Unrotate
View(Hair)
summary(SalesFImage)
# Exploratory Data Analysis
# Univariate Analysis
## Frequency Distribution of each Independent numerical Variable
par(mfrow=c(2,2))
hist(ProdQual,col = "yellow",breaks = 20,main = "Histogram Of Product Quality",xlab = expression(bold(ProdQual)))
# From the plot we can see that the the Product Quality ratings distribution lies in the range 5 - 10.
# Most of the Product Quality got ratings around 9-9.5.It means most of the Product Quality is Very good.
# Another important thing to note is its not normally Distributed.
hist(Ecom,col = "Blue",breaks = 20,main = "Histogram Of E-Commerce",xlab = expression(bold(Ecom)))
# From the plot we can see that the the Product Quality ratings distribution lies in the range 5 - 10.
# Most of the Product Quality got ratings around 9-9.5.It means most of the Product Quality is Very good.
# Another important thing to note is its not normally Distributed.
hist(Hair$Ecom,col = "Blue",breaks = 20,main = "Histogram Of E-Commerce",xlab = expression(bold(Ecom)))
# Here,the Ecom rating distribution lies in the range of 2- 6.
# Its clear that the Ecom services provided is not good as we dont have a single rating above 6.
# It is almost normally distributed.
hist(TechSup,col = "Red",main = "Histogram of Technical Support",xlab = expression(bold(TechSup)))
# Here, the Tech Support ratings distribution range lies from 1 to 8.5 as most values tend to be around 5-7
# From this we can infer that the rating  for Technical Support is pretty average and have a even spread.
hist(CompRes,col = "Green",main = "Histogram of Complaint Resolution",xlab = expression(bold(CompRes)))
boxplot(Hair[,2:13],col=rainbow(length(Hair[,2:13])),las=2)
# Outlier Detection
par(mfrow=c(1,1))
boxplot(Hair[,2:13],col=rainbow(length(Hair[,2:13])),las=2)
# From the Plot its Clear that the Variables E-Commerce,Sales Force Image,Order and Billing,Delivery Speed has Outliers,
# Which we also assumed in Histogram Plot Itself.
# We need to deal with the outlier as it will affect our Model interprettability/Validity.
# We need to find the Outlier values using the formula (Q1-1.5IQR,Q1+1.5IQR)
# Outlier Treatment
# There are different methods in dealing with Outliers
# One Of the method is Winsorizing technique which replaces the outliers with the allowable or Benchmark Range(Q1-1.5IQR,Q1+1.5IQR).
summary(Hair$Ecom)
par(mfrow=c(2,2))
hist(ProdQual,col = "yellow",breaks = 20,main = "Histogram Of Product Quality",xlab = expression(bold(ProdQual)))
hist(Hair$Ecom,col = "Blue",breaks = 20,main = "Histogram Of E-Commerce",xlab = expression(bold(Ecom)))
hist(TechSup,col = "Red",main = "Histogram of Technical Support",xlab = expression(bold(TechSup)))
hist(CompRes,col = "Green",main = "Histogram of Complaint Resolution",xlab = expression(bold(CompRes)))
# Exploratory Data Analysis
# Univariate Analysis
## Frequency Distribution of each Independent numerical Variable
par(mfrow=c(2,2))
hist(ProdQual,col = "yellow",breaks = 20,main = "Histogram Of Product Quality",xlab = expression(bold(ProdQual)))
# From the plot we can see that the the Product Quality ratings distribution lies in the range 5 - 10.
# Most of the Product Quality got ratings around 9-9.5.It means most of the Product Quality is Very good.
# Another important thing to note is its not normally Distributed.
hist(Hair$Ecom,col = "Blue",breaks = 20,main = "Histogram Of E-Commerce",xlab = expression(bold(Ecom)))
# Here,the Ecom rating distribution lies in the range of 2- 6.
# Its clear that the Ecom services provided is not good as we dont have a single rating above 6.
# It is almost normally distributed.
hist(TechSup,col = "Red",main = "Histogram of Technical Support",xlab = expression(bold(TechSup)))
# Here, the Tech Support ratings distribution range lies from 1 to 8.5 as most values tend to be around 5-7
# From this we can infer that the rating  for Technical Support is pretty average and have a even spread.
hist(CompRes,col = "Green",main = "Histogram of Complaint Resolution",xlab = expression(bold(CompRes)))
pairs(Cor,lower.panel = panel.smooth,cex = .8)
# Let's Check the Clustering of our Factors Visually
plot(Rotate,row.names(Rotate$loadings),cex=1.0)
unlink('Factor-Hair-Revised_cache', recursive = TRUE)
